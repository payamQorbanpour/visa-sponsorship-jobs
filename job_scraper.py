#!/usr/bin/env python3
"""
Job Scraper for Visa Sponsorship Positions
Searches multiple job boards for DevOps/SRE roles with visa sponsorship
"""

import argparse
import csv
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set

import pandas as pd
import yaml
from jobspy import scrape_jobs


class JobScraper:
    """Main job scraper class"""
    
    DEFAULT_CONFIG = {
        'job_roles': ['DevOps Engineer', 'Site Reliability Engineer'],
        'countries': ['germany', 'netherlands', 'sweden', 'spain', 'belgium', 'austria'],
        'job_sites': {
            'priority': ['indeed', 'glassdoor'],
            'secondary': ['linkedin', 'google', 'zip_recruiter'],
            'disabled': []
        },
        'visa_keywords': [
            'visa sponsorship',
            'visa',
            'visa support',
            'relocation package',
            'relocation assistance',
            'work permit',
            'sponsorship available',
            'relocation'
        ],
        'search_params': {
            'results_per_site': 50,
            'job_type': 'fulltime',
            'is_remote': False,
            'hours_old': 168,  # 7 days
            'distance': 50
        },
        'output': {
            'format': 'csv',
            'directory': 'results',
            'filename_pattern': 'jobs_{timestamp}.csv'
        },
        'exclusion_keywords': [
            'national of an EU member state',
            'EU member state national',
            'EU citizen',
            'European Union citizen',
            'EU citizenship required',
            'must be an EU national',
            'EU passport required',
            'citizenship of an EU country',
            'only EU nationals',
            'restricted to EU citizens',
            'EU/EEA nationals only',
            'EEA nationals only',
            'Swiss nationals only'
        ],
        'filters': {
            'visa_sponsorship_filter': True,
            'exclusion_filter': True,
            'case_sensitive': False
        }
    }
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize scraper with config"""
        self.config = self.DEFAULT_CONFIG.copy()
        
        if config_path and os.path.exists(config_path):
            self.load_config(config_path)
        
        self.results = []
        self.stats = {
            'total_scraped': 0,
            'after_filter': 0,
            'by_site': {},
            'by_country': {}
        }
    
    def load_config(self, config_path: str):
        """Load configuration from YAML file"""
        try:
            with open(config_path, 'r') as f:
                user_config = yaml.safe_load(f)
            
            # Deep merge configs
            self._deep_update(self.config, user_config)
            print(f"âœ“ Loaded configuration from {config_path}")
        except Exception as e:
            print(f"âš  Warning: Could not load config file: {e}")
            print("Using default configuration")
    
    def _deep_update(self, base_dict: dict, update_dict: dict):
        """Recursively update nested dictionaries"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def get_enabled_sites(self) -> List[str]:
        """Get list of enabled job sites"""
        priority = self.config['job_sites']['priority']
        secondary = self.config['job_sites']['secondary']
        disabled = set(self.config['job_sites']['disabled'])
        
        all_sites = priority + secondary
        enabled = [site for site in all_sites if site not in disabled]
        
        return enabled
    
    def compile_visa_keywords_pattern(self) -> re.Pattern:
        """Compile regex pattern for visa sponsorship keywords"""
        keywords = self.config['visa_keywords']
        # Escape special regex characters and join with OR
        escaped = [re.escape(kw) for kw in keywords]
        pattern = '|'.join(escaped)
        
        flags = re.IGNORECASE if not self.config['filters']['case_sensitive'] else 0
        return re.compile(pattern, flags)
    
    def compile_exclusion_keywords_pattern(self) -> re.Pattern:
        """Compile regex pattern for exclusion keywords"""
        keywords = self.config.get('exclusion_keywords', [])
        if not keywords:
            return None
        
        # Escape special regex characters and join with OR
        escaped = [re.escape(kw) for kw in keywords]
        pattern = '|'.join(escaped)
        
        flags = re.IGNORECASE if not self.config['filters']['case_sensitive'] else 0
        return re.compile(pattern, flags)
    
    def filter_by_exclusion(self, jobs_df: pd.DataFrame) -> pd.DataFrame:
        """Filter out jobs containing exclusion keywords (e.g., EU citizenship requirements)"""
        if not self.config['filters'].get('exclusion_filter', False):
            print("â„¹ Exclusion filter is disabled")
            return jobs_df
        
        if jobs_df.empty:
            return jobs_df
        
        pattern = self.compile_exclusion_keywords_pattern()
        if pattern is None:
            return jobs_df
        
        print(f"\nğŸš« Filtering out jobs with exclusion keywords...")
        print(f"   Excluding: EU citizenship requirements, etc.")
        
        original_count = len(jobs_df)
        
        # Check if description column exists
        if 'description' not in jobs_df.columns:
            print("âš  Warning: No description column found, skipping exclusion filter")
            return jobs_df
        
        # Fill NaN descriptions with empty string
        jobs_df['description'] = jobs_df['description'].fillna('')
        
        # Apply exclusion filter (keep jobs that DON'T match exclusion patterns)
        mask = ~jobs_df['description'].str.contains(pattern, na=False, regex=True)
        filtered_df = jobs_df[mask].copy()
        
        filtered_count = len(filtered_df)
        excluded_count = original_count - filtered_count
        print(f"   âœ“ Excluded {excluded_count} jobs with citizenship requirements ({filtered_count} remaining)")
        
        return filtered_df
    
    def filter_by_visa_sponsorship(self, jobs_df: pd.DataFrame) -> pd.DataFrame:
        """Filter jobs by visa sponsorship keywords in description"""
        if not self.config['filters']['visa_sponsorship_filter']:
            print("â„¹ Visa sponsorship filter is disabled")
            return jobs_df
        
        if jobs_df.empty:
            return jobs_df
        
        print(f"\nğŸ” Filtering for visa sponsorship keywords...")
        print(f"   Keywords: {', '.join(self.config['visa_keywords'][:3])}...")
        
        pattern = self.compile_visa_keywords_pattern()
        
        # Filter by description
        original_count = len(jobs_df)
        
        # Check if description column exists and is not null
        if 'description' not in jobs_df.columns:
            print("âš  Warning: No description column found, skipping visa filter")
            return jobs_df
        
        # Fill NaN descriptions with empty string
        jobs_df['description'] = jobs_df['description'].fillna('')
        
        # Apply filter
        mask = jobs_df['description'].str.contains(pattern, na=False, regex=True)
        filtered_df = jobs_df[mask].copy()
        
        filtered_count = len(filtered_df)
        print(f"   âœ“ Found {filtered_count} jobs with visa sponsorship ({original_count - filtered_count} filtered out)")
        
        # Add a flag column
        filtered_df['visa_sponsorship_mentioned'] = True
        
        return filtered_df
    
    def scrape_jobs_for_country(self, country: str, role: str) -> pd.DataFrame:
        """Scrape jobs for a specific country and role"""
        enabled_sites = self.get_enabled_sites()
        
        if not enabled_sites:
            print("âš  No job sites enabled!")
            return pd.DataFrame()
        
        print(f"\n{'='*60}")
        print(f"ğŸŒ Country: {country.upper()}")
        print(f"ğŸ’¼ Role: {role}")
        print(f"ğŸ”— Sites: {', '.join(enabled_sites)}")
        print(f"{'='*60}")
        
        try:
            jobs_df = scrape_jobs(
                site_name=enabled_sites,
                search_term=role,
                location=country,
                distance=self.config['search_params']['distance'],
                is_remote=self.config['search_params']['is_remote'],
                job_type=self.config['search_params']['job_type'],
                results_wanted=self.config['search_params']['results_per_site'],
                hours_old=self.config['search_params']['hours_old'],
                country_indeed=country,
                description_format='markdown',  # Use markdown for descriptions (plain not supported)
                verbose=1
            )
            
            if not jobs_df.empty:
                # Add metadata
                jobs_df['search_country'] = country
                jobs_df['search_role'] = role
                jobs_df['scraped_at'] = datetime.now().isoformat()
                
                print(f"âœ“ Scraped {len(jobs_df)} jobs")
                
                # Update stats
                self.stats['total_scraped'] += len(jobs_df)
                self.stats['by_country'][country] = self.stats['by_country'].get(country, 0) + len(jobs_df)
                
                for site in jobs_df['site'].unique():
                    self.stats['by_site'][site] = self.stats['by_site'].get(site, 0) + len(jobs_df[jobs_df['site'] == site])
            
            return jobs_df
        
        except Exception as e:
            print(f"âŒ Error scraping {country} for {role}: {str(e)}")
            return pd.DataFrame()
    
    def scrape_all(self) -> pd.DataFrame:
        """Scrape all configured countries and roles"""
        all_jobs = []
        
        total_combinations = len(self.config['countries']) * len(self.config['job_roles'])
        current = 0
        
        print(f"\nğŸš€ Starting job search...")
        print(f"   Roles: {len(self.config['job_roles'])}")
        print(f"   Countries: {len(self.config['countries'])}")
        print(f"   Total searches: {total_combinations}")
        
        for country in self.config['countries']:
            for role in self.config['job_roles']:
                current += 1
                print(f"\n[{current}/{total_combinations}] ", end="")
                
                jobs_df = self.scrape_jobs_for_country(country, role)
                
                if not jobs_df.empty:
                    all_jobs.append(jobs_df)
        
        if not all_jobs:
            print("\nâš  No jobs found!")
            return pd.DataFrame()
        
        # Combine all results
        combined_df = pd.concat(all_jobs, ignore_index=True)
        
        # Remove duplicates based on job_url
        original_count = len(combined_df)
        combined_df = combined_df.drop_duplicates(subset=['job_url'], keep='first')
        duplicates_removed = original_count - len(combined_df)
        
        if duplicates_removed > 0:
            print(f"\nğŸ”„ Removed {duplicates_removed} duplicate job listings")
        
        print(f"\nğŸ“Š Total unique jobs scraped: {len(combined_df)}")
        
        return combined_df
    
    def save_results(self, jobs_df: pd.DataFrame, output_path: Optional[str] = None, suffix: str = ''):
        """Save results to file"""
        if jobs_df.empty:
            print("\nâš  No jobs to save!")
            return
        
        # Create output directory
        output_dir = Path(self.config['output']['directory'])
        output_dir.mkdir(exist_ok=True)
        
        # Generate filename
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = self.config['output']['filename_pattern'].format(timestamp=timestamp)
            # Add suffix if provided
            if suffix:
                filename = filename.replace('.csv', f'{suffix}.csv')
            output_path = output_dir / filename
        elif suffix:
            # Add suffix to provided path
            output_path = Path(str(output_path).replace('.csv', f'{suffix}.csv'))
        
        # Drop description column to prevent CSV formatting issues
        if 'description' in jobs_df.columns:
            jobs_df = jobs_df.drop(columns=['description'])
        
        # Reorder columns for better readability
        priority_columns = [
            'site', 'title', 'company', 'location', 'date_posted',
            'job_type', 'job_url', 'search_country', 
            'search_role', 'visa_sponsorship_mentioned', 'note'
        ]
        
        # Keep only existing columns
        existing_priority = [col for col in priority_columns if col in jobs_df.columns]
        other_columns = [col for col in jobs_df.columns if col not in existing_priority]
        ordered_columns = existing_priority + other_columns
        
        jobs_df = jobs_df[ordered_columns]
        
        # Save based on format
        output_format = self.config['output']['format'].lower()
        
        if output_format == 'csv':
            jobs_df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)
            print(f"\nâœ… Results saved to: {output_path}")
        elif output_format == 'json':
            output_path = Path(str(output_path).replace('.csv', '.json'))
            jobs_df.to_json(output_path, orient='records', indent=2)
            print(f"\nâœ… Results saved to: {output_path}")
        elif output_format == 'excel':
            output_path = Path(str(output_path).replace('.csv', '.xlsx'))
            jobs_df.to_excel(output_path, index=False)
            print(f"\nâœ… Results saved to: {output_path}")
        else:
            print(f"âš  Unknown format: {output_format}, saving as CSV")
            jobs_df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)
        
        # Save separate files per site if requested
        if len(jobs_df['site'].unique()) > 1:
            print(f"\nğŸ“‘ Saving separate files by site...")
            for site in jobs_df['site'].unique():
                site_df = jobs_df[jobs_df['site'] == site]
                site_filename = str(output_path).replace('.csv', f'_{site}.csv')
                site_df.to_csv(site_filename, index=False, quoting=csv.QUOTE_ALL)
                print(f"   âœ“ {site}: {len(site_df)} jobs â†’ {Path(site_filename).name}")
    
    def print_statistics(self):
        """Print scraping statistics"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ STATISTICS")
        print(f"{'='*60}")
        print(f"Total jobs scraped: {self.stats['total_scraped']}")
        print(f"After filtering: {self.stats['after_filter']}")
        
        if self.stats['by_site']:
            print(f"\nğŸ“Š By Site:")
            for site, count in sorted(self.stats['by_site'].items(), key=lambda x: x[1], reverse=True):
                print(f"   {site}: {count}")
        
        if self.stats['by_country']:
            print(f"\nğŸŒ By Country:")
            for country, count in sorted(self.stats['by_country'].items(), key=lambda x: x[1], reverse=True):
                print(f"   {country}: {count}")
        
        print(f"{'='*60}")
    
    def run(self, output_path: Optional[str] = None):
        """Main execution method"""
        print("\n" + "="*60)
        print("ğŸ” JOB SCRAPER FOR VISA SPONSORSHIP POSITIONS")
        print("="*60)
        
        # Scrape jobs
        jobs_df = self.scrape_all()
        
        if jobs_df.empty:
            return
        
        # Filter by visa sponsorship
        filtered_df = self.filter_by_visa_sponsorship(jobs_df)
        
        # Apply exclusion filter (remove jobs requiring EU citizenship)
        filtered_df = self.filter_by_exclusion(filtered_df)
        
        self.stats['after_filter'] = len(filtered_df)
        
        # Save results
        if filtered_df.empty and not jobs_df.empty:
            # No results after filtering, but we have unfiltered results
            print("\nâš ï¸  No jobs found with visa sponsorship keywords")
            print("ğŸ’¾ Saving unfiltered results for manual review...")
            
            # Add a note column
            jobs_df['note'] = 'No visa keywords found - manual review needed'
            self.save_results(jobs_df, output_path, suffix='_unfiltered')
        elif not filtered_df.empty:
            # Save filtered results
            self.save_results(filtered_df, output_path)
            
            # Also save unfiltered if different
            if len(filtered_df) < len(jobs_df):
                print(f"\nğŸ’¾ Also saving unfiltered results ({len(jobs_df)} total jobs)...")
                jobs_df['note'] = 'Unfiltered - may not have visa keywords'
                self.save_results(jobs_df, output_path, suffix='_all_jobs')
        
        # Print stats
        self.print_statistics()


def interactive_mode():
    """Run scraper in interactive mode"""
    print("\nğŸ¤– INTERACTIVE JOB SCRAPER")
    print("="*60)
    
    # Ask for job roles
    print("\n1ï¸âƒ£  Job Roles")
    roles_input = input("Enter job roles (comma-separated) [DevOps Engineer,Site Reliability Engineer]: ").strip()
    roles = [r.strip() for r in roles_input.split(',')] if roles_input else ['DevOps Engineer', 'Site Reliability Engineer']
    
    # Ask for countries
    print("\n2ï¸âƒ£  Countries")
    print("Available: germany, netherlands, sweden, spain, belgium, austria")
    countries_input = input("Enter countries (comma-separated) [all]: ").strip()
    if countries_input.lower() in ['all', '']:
        countries = ['germany', 'netherlands', 'sweden', 'spain', 'belgium', 'austria']
    else:
        countries = [c.strip().lower() for c in countries_input.split(',')]
    
    # Ask for job sites
    print("\n3ï¸âƒ£  Job Sites")
    print("Available: indeed, glassdoor, linkedin, google, zip_recruiter")
    sites_input = input("Enter sites to EXCLUDE (comma-separated) [none]: ").strip()
    disabled_sites = [s.strip().lower() for s in sites_input.split(',')] if sites_input else []
    
    # Ask for results per site
    print("\n4ï¸âƒ£  Results")
    results_input = input("Results per site [50]: ").strip()
    results_per_site = int(results_input) if results_input.isdigit() else 50
    
    # Ask for visa filter
    print("\n5ï¸âƒ£  Visa Sponsorship Filter")
    visa_filter = input("Enable visa sponsorship filter? [Y/n]: ").strip().lower()
    visa_filter_enabled = visa_filter != 'n'
    
    # Ask for days old
    print("\n6ï¸âƒ£  Job Age")
    days_input = input("Max days old [7]: ").strip()
    days_old = int(days_input) if days_input.isdigit() else 7
    
    # Create config
    config = JobScraper.DEFAULT_CONFIG.copy()
    config['job_roles'] = roles
    config['countries'] = countries
    config['job_sites']['disabled'] = disabled_sites
    config['search_params']['results_per_site'] = results_per_site
    config['filters']['visa_sponsorship_filter'] = visa_filter_enabled
    config['search_params']['hours_old'] = days_old * 24
    
    print("\n" + "="*60)
    print("Configuration:")
    print(f"  Roles: {', '.join(roles)}")
    print(f"  Countries: {', '.join(countries)}")
    print(f"  Excluded sites: {', '.join(disabled_sites) if disabled_sites else 'none'}")
    print(f"  Results per site: {results_per_site}")
    print(f"  Visa filter: {'enabled' if visa_filter_enabled else 'disabled'}")
    print(f"  Max age: {days_old} days")
    print("="*60)
    
    confirm = input("\nProceed? [Y/n]: ").strip().lower()
    if confirm == 'n':
        print("Cancelled.")
        return
    
    # Run scraper
    scraper = JobScraper()
    scraper.config = config
    scraper.run()


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Job Scraper for Visa Sponsorship Positions',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Use config file
  python job_scraper.py --config config.yaml
  
  # CLI arguments
  python job_scraper.py --roles "DevOps Engineer" "SRE" --countries germany sweden
  
  # Interactive mode
  python job_scraper.py --interactive
  
  # Exclude specific sites
  python job_scraper.py --exclude-sites linkedin google --no-visa-filter
  
  # Change results per site
  python job_scraper.py --results 100 --days 14
        """
    )
    
    parser.add_argument('-c', '--config', help='Path to config YAML file')
    parser.add_argument('-i', '--interactive', action='store_true', help='Run in interactive mode')
    parser.add_argument('-r', '--roles', nargs='+', help='Job roles to search for')
    parser.add_argument('--countries', nargs='+', help='Countries to search in')
    parser.add_argument('--exclude-sites', nargs='+', help='Job sites to exclude')
    parser.add_argument('--results', type=int, help='Results per site (default: 50)')
    parser.add_argument('--days', type=int, help='Max job age in days (default: 7)')
    parser.add_argument('--no-visa-filter', action='store_true', help='Disable visa sponsorship filter')
    parser.add_argument('-o', '--output', help='Output file path')
    parser.add_argument('--format', choices=['csv', 'json', 'excel'], help='Output format')
    
    args = parser.parse_args()
    
    # Interactive mode
    if args.interactive:
        interactive_mode()
        return
    
    # Load config or use defaults
    scraper = JobScraper(args.config)
    
    # Override with CLI arguments
    if args.roles:
        scraper.config['job_roles'] = args.roles
    
    if args.countries:
        scraper.config['countries'] = [c.lower() for c in args.countries]
    
    if args.exclude_sites:
        scraper.config['job_sites']['disabled'] = [s.lower() for s in args.exclude_sites]
    
    if args.results:
        scraper.config['search_params']['results_per_site'] = args.results
    
    if args.days:
        scraper.config['search_params']['hours_old'] = args.days * 24
    
    if args.no_visa_filter:
        scraper.config['filters']['visa_sponsorship_filter'] = False
    
    if args.format:
        scraper.config['output']['format'] = args.format
    
    # Run scraper
    scraper.run(args.output)


if __name__ == '__main__':
    main()
